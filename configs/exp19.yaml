datasets:
    data_path: '/srv/scratch/z5369417/outputs/phonemization_speechocean_exp11_4' #should be a DatasetDict
    train_part: 'train'
    validation_part: 'test'
    test_part: 'test,test' #, test.other'
    cache_dir: '/home/z5369417/.cache'

phonological:
    attribute_list_file: 'data/list_attributes-camb.txt'
    phoneme2att_map_file: 'data/Phoneme2att_camb_att_noDiph.csv'
    phonetic_alphabet: 'arpa' #Should be column name in phoneme2att_map_file named as "Phoneme_{phonetic_alphabet}"

preprocessor:
    sampling_rate: 16000
    do_normalize: true
    return_attention_mask: false
    phoneme_column: 'actual_spoken_phonemes'
    do_phonemize: false
    num_proc: 1 #Number of processors for preprocessing the dataset. Number of CPUs
    max_length_in_sec: 15 #Speech files longer than 15 seconds will be filtered out due to CUDA out-of-memory error
    save_preprocessed_data: true #will be saved in working_dir/preprocessed_data
    load_from_preprocessed_data: true #will be loaded from working_dir/preprocessed_data if exist else perprocess it
    decouple_diphthongs: true
    diphthongs_to_monophthongs_map_file: 'data/Diphthongs_en_us-arpa.csv'

training:
    model_path: 'facebook/wav2vec2-large-robust'
    gradient_checkpointing: true
    ctc_loss_reduction: 'mean'
    freeze_feature_encoder: true
    group_by_length: true
    train_batch_size: 32
    evaluation_strategy: 'steps'
    enable_fp16: true
    num_train_epochs: 10
    save_steps: 100
    logging_steps: 100
    prediction_loss_only: true
    learning_rate: 1e-04
    weight_decay: 0.005
    warmup_ratio: 0.1
    load_best_model_at_end: true
    save_total_limit: 3

evaluation:
    #trained_model_path: #It's by default in working_dir/fine_tune/best
    spaces_between_special_tokens: true
    metric_path: 'wer'
    eval_extra_data: '/g/data/iv96/mostafa/datasets/librispeech_clean_dp'
    eval_extra_data_parts: 'test,validation'
    auto_eval: true


output:
        working_dir: '/srv/scratch/z5369417/outputs/trained_result/speechocean/exp19'
